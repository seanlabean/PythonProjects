{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning\n",
    "\n",
    "An attempt to repurpose the basic structure of the decision tree classifier employed in the livestream from Great Learning: \n",
    "\n",
    "https://www.youtube.com/watch?v=tdFMIO5lfgA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using pandas dataframes and numpy matrix manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fullprint(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Unsets numpy's array truncation to print full arrays.\n",
    "    Returns to default numpy truncation before exiting.\n",
    "    https://stackoverflow.com/questions/1987694/how-to-print-the-full-numpy-array-without-truncation/24542498#24542498\n",
    "    \"\"\"\n",
    "    from pprint import pprint\n",
    "    import numpy\n",
    "    opt = numpy.get_printoptions()\n",
    "    numpy.set_printoptions(threshold=numpy.inf)\n",
    "    pprint(*args, **kwargs)\n",
    "    numpy.set_printoptions(**opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "Fake and Real news data files downloaded via Kaggle user Clément Bisaillon: \n",
    "\n",
    "https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our csv files.\n",
    "fake = pd.read_csv('./Fake.csv')\n",
    "real = pd.read_csv('./True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what the data sets look like. \n",
    "# title, text, subject, date\n",
    "# True.csv looks the same way btw.\n",
    "fake.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolating and Cleaning Data\n",
    "We want to drop nan entries and slice csvs to include as many data entries as we want.\n",
    "* There are upwards of 20k entries each, 1k is good for testing.\n",
    "\n",
    "**Reminder:** a \"data entry\" at this point is a title, text, subject, and date\n",
    "\n",
    "#### Independent data\n",
    "Then, isolate just the independent variable information from the real and fake datasets.\n",
    "* This constitutes the article Titles and Texts.\n",
    "\n",
    "We will need to have all independent variables in a single parsable object. Numpy's `concatenate` will do the trick with axis=0 to avoid array flattenting.\n",
    "\n",
    "We are left with an array of 2 columns (title, text) with len(independent_fake)+len(independent_real) rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and slice data \n",
    "fake = fake.dropna()[0:5000]\n",
    "real = real.dropna()[0:5000]\n",
    "\n",
    "# Isolate independent data points\n",
    "independent_fake = fake.iloc[:,:-2].values\n",
    "independent_real = real.iloc[:,:-2].values\n",
    "\n",
    "# Combine arrays\n",
    "X = np.concatenate((independent_fake,independent_real),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependent data\n",
    "\n",
    "We now need to develop a category array to label entries as real or fake.\n",
    "\n",
    "The easiest way to do this is generate numpy arrays of 1's for \"fake\" and 0's for \"real\". Then concatenate in the same way we did the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_fake = np.ones(len(independent_fake)) # fake == 1\n",
    "dependent_real = np.zeros(len(independent_real)) # real == 0\n",
    "\n",
    "y = np.concatenate((dependent_fake,dependent_real),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data\n",
    "We need to process our independent data (titles, texts) into numerics using a CountVectorizer object.\n",
    "\n",
    "How a `CountVectorizer` works:\n",
    "* When fit to a dataset, the CountVectorizer will extract all instances of a datapoint appearing in a dataset.\n",
    "    * In this case, a datapoint is a word and the dataset is an article's title or body text.\n",
    "* When transformed to the fit, the CountVectorizer will output array of occurrences of each datapoint per entry.\n",
    "    * A.k.a it tells use how many times each word appears in a particular article.\n",
    "    \n",
    "The `.get_features()` attribute will show the unique datapoints that have been extracted.\n",
    "    \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Generate the occurrence array for the body text.\n",
    "cv_body = CountVectorizer()\n",
    "\n",
    "mat_body = cv_body.fit_transform(X[:,1]).todense() # body text occurrence array\n",
    "# print(cv_body.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate the occurrence array for the titles.\n",
    "cv_head = CountVectorizer()\n",
    "mat_head = cv_head.fit_transform(X[:,0]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stick the occurrence arrays together (title matched with body text)\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.hstack.html\n",
    "X_mat = np.hstack((mat_head, mat_body))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate training and testing data\n",
    "This is a critical step in machine learning. \n",
    "We need to separate the independent and dependent data into train. This is made very easy for us by the `train_test_split()` sklearn function. We can define how much of our data we want to be train vs. test too.\n",
    "\n",
    "**Recall:** X_mat is our organized and processed independent data. y is our organized dependent array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "Now we can finally make our machine learn something!\n",
    "\n",
    "We are using a decision tree that will decide to classify a data entry as a member of the categories 'real' or 'fake' based on that entry's similarity to other entries.\n",
    "\n",
    "I *think* that the 'entropy' criterion has the decision tree sort data into categories whos independent data is most similar. So for us I believe that means that we are essentially sorting real and fake news by word usage, making the assertion (assumption) that all fake news uses similar words in their titles and texts compared to other fake news. Also that all real news uses similar words in similar frequency.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create decision tree object. \n",
    "# Fit the training data to the ML object.\n",
    "dtc = DecisionTreeClassifier(criterion='entropy')\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Generate dependent data (real/fake) from test independent data\n",
    "y_pred = dtc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Accuracy\n",
    "\n",
    "The `confusion_matrix()` function in sklearn allows us to compare the ACTUAL dependent variables (whether a tested article is real or fake) with the predicted category from our algorithm.\n",
    "\n",
    "In this case, the array will be 2x2 since there are two categories (real & fake) with two possible predictive outcomes (correctly or incorrectly identifying). \n",
    "\n",
    "array([[# correctly labeled fake, # incorrectly labeled fake], \n",
    "\n",
    "[# correctly labeled true, # incorrectly labeled true]])\n",
    "\n",
    "To determine an algorithm's effectiveness, sum the decending diagonal (total correctly categorized datapoints) and divide by sum of all matrix values (total # categorizations, correct and incorrect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1011,    1],\n",
       "       [   0,  988]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compare our dependent variable prediction with the actual values\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Learned Model on New Data\n",
    "\n",
    "Let's see if our super smart fake news catagorizer can look at new entries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
